\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}

\title{Neural Network Backpropagation}
\author{Mehmet}
\date{}

\begin{document}

\maketitle

To compute the gradients, we'll work backwards:

\begin{enumerate}
    \item Compute the error at the output layer:
    \begin{equation*}
        \boldsymbol{\delta}_2 = (\mathbf{a}_2 - \mathbf{y}) \odot \sigma'(\mathbf{z}_2)
    \end{equation*}
    where $\odot$ is element-wise multiplication and $\sigma'$ is the derivative of the sigmoid function.

    \item Compute gradients for $\mathbf{W}_2$ and $\mathbf{b}_2$:
    \begin{align*}
        \frac{\partial L}{\partial \mathbf{W}_2} &= \mathbf{a}_1^T \boldsymbol{\delta}_2 \\
        \frac{\partial L}{\partial \mathbf{b}_2} &= \sum \boldsymbol{\delta}_2 \quad \text{(sum over the batch dimension)}
    \end{align*}

    \item Backpropagate the error to the hidden layer:
    \begin{equation*}
        \boldsymbol{\delta}_1 = (\boldsymbol{\delta}_2 \mathbf{W}_2^T) \odot \sigma'(\mathbf{z}_1)
    \end{equation*}

    \item Compute gradients for $\mathbf{W}_1$ and $\mathbf{b}_1$:
    \begin{align*}
        \frac{\partial L}{\partial \mathbf{W}_1} &= \mathbf{a}_0^T \boldsymbol{\delta}_1 \\
        \frac{\partial L}{\partial \mathbf{b}_1} &= \sum \boldsymbol{\delta}_1 \quad \text{(sum over the batch dimension)}
    \end{align*}
\end{enumerate}

The gradient of the sigmoid function is:
\begin{equation*}
    \sigma'(x) = \sigma(x) (1 - \sigma(x))
\end{equation*}

These gradients can then be used to update the weights and biases using an optimization algorithm like gradient descent:

\begin{align*}
    \mathbf{W} &= \mathbf{W} - \eta \frac{\partial L}{\partial \mathbf{W}} \\
    \mathbf{b} &= \mathbf{b} - \eta \frac{\partial L}{\partial \mathbf{b}}
\end{align*}

where $\eta$ is the learning rate.

Partial equation for a given weight then derived like this:
\begin{equation*}
  \frac{\partial C}{\partial {w_1}} = \frac{\partial C}{\partial {a_i}} \frac{\partial {a_i}}{\partial b} \frac{\partial b}{\partial x} \frac{\partial x}{\partial {w_1}}
\end{equation*}
\section{One Neuron Cost}
A single neuron and input forwarding contains the multiplication of a weight and a input, the addition of bias, and the activation function used to get the output data.
\begin{align}
  z         &= w_1 x_1 + b \\
  f(w_1, b) &= \sigma(z)
\end{align}
where the activation function is a non-linear logistic function.
\begin{equation}
\sigma(x) = \frac{1}{1+{e^-x}}
\end{equation}
\begin{equation}
C(w_1) = \frac{1}{2}\displaystyle\sum \left(\sigma(z) - y_1 \right)^2
\end{equation}

\end{document}
